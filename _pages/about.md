---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Senior Research Scientist at [ServiceNow Research](https://www.servicenow.com/research/), specializing in post-training methods for computer-use agents. I see computer use as the ultimate playground for testing agents, thanks to its ubiquity and diversity. 
 My research involves conducting large-scale empirical studies to systematically evaluate trade-offs among different approaches and to develop practical know-how, with reinforcement learning being a particular focus.

As a core contributor to the web-agent research library ecosystem, I actively shape evaluation frameworks ([BrowserGym](https://github.com/ServiceNow/BrowserGym), [WorkArena](https://github.com/ServiceNow/WorkArena)) and development platforms ([AgentLab](https://github.com/ServiceNow/AgentLab)). My goal is to bridge foundational research and scalable tools to advance the field.

Previously, I completed my Ph.D. at the Quebec Artificial Intelligence Institute ([Mila](https://mila.quebec/en/)) under Professor [Laurent Charlin](http://www.cs.toronto.edu/~lcharlin/). During my doctoral studies, I collaborated with [DeepMind](https://deepmind.com/)’s Continual Learning team led by [Marc'Aurelio Ranzato](https://ranzato.github.io/), [Amazon](https://www.amazon.science/)’s team under [Alex Smola](https://alex.smola.org/), and ElementAI prior to its integration with ServiceNow.

My Ph.D. research focused on building agents capable of accumulating and transferring knowledge across tasks, drawing from continual learning, transfer learning, and meta-learning. My work explored applications in language, vision, and reinforcement learning, emphasizing improvements in data and compute efficiency.


# News

* **07/2023** Our new large-scale continual learning benchmark [Nevis'22](https://jmlr.org/papers/v24/22-1345.html) was accepted at <ins>JMLR</ins>! Work done while interning at DeepMind. 

* **05/2023** Our work on [task-agnostic continual RL](https://arxiv.org/abs/2205.14495) is accepted at  <ins>CoLLAs 2023</ins>! Checkout the [5-min video summary](https://www.youtube.com/watch?v=T0RYCuECAuw&t). 

* **12/2021** Our [comparative study of large language models in continual learning](https://openreview.net/forum?id=figzpGMrdD) is accepted at <ins>ICLR 2022</ins>!

* **09/2021** Our work digging into gradient sparsity for meta and continual learning, [Sparse-MAML](https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html), is accepted at <ins>NeurIPS 2021</ins>!

* **09/2021** Our work solving the task-inference problem in compositonal continual learning, [Local Modle Composition](https://proceedings.neurips.cc/paper/2021/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html), is accepted at <ins>NeurIPS 2021</ins>!

* **07/2021** Our work introducing [DiVE](https://arxiv.org/abs/2103.10226), a counterfactual explanation method that goes beyond generating trivial counterfactuals, is accepted at <ins>ICCV 2021</ins>! 

* **09/2020** Our work proposing a new approach to continual learning [(OSAKA)](https://arxiv.org/abs/2003.05856) is accepted at <ins>NeurIPS 2020</ins>! 

* **09/2020** Our work proposing a synthetic dataset generator [(Synbols)](https://arxiv.org/abs/2009.06415) to probe different learning algorithms is accepted at <ins>NeurIPS 2020</ins>!

* **08/2020** I gave [a talk](https://youtu.be/AHGiF21WZbw) on our new approach to continual-learning evaluation towards real-life deployment of continual-learning systems [(OSAKA)](https://arxiv.org/abs/2003.05856) at [ContinualAI](https://www.continualai.org/)

* **06/2020** I hosted a panel with Chelsea Finn, Chris Kanan and Subutai Ahmad at our Continual Learning workshop at CVPR 2020! You can find it [here](https://www.youtube.com/watch?v=sp3Y9Np25Og&t)

* **06/2020** Our work on [online continual compression](https://arxiv.org/abs/1911.08019) lead by [my brother](https://www.cs.mcgill.ca/~lpagec/) is accepted <ins>ICML 2020</ins>! You can find a 18-min video [here](https://icml.cc/virtual/2020/poster/6338)

* **12/2019** Our workshop on [Continual Learning at CVPR2020](https://sites.google.com/view/clvision2020) as been accepted! Watch out for our really cool competition.

* **12/2019** Our work on [Language GANs Falling Short](https://arxiv.org/abs/1811.02549) was accepted <ins>ICLR 2020</ins>! You can find a 5-min video summary [here](https://iclr.cc/virtual_2020/poster_BJgza6VtPB.html)

* **09/2019** Our work on [Online Continual learning with Maximal Interfered Retrieval](http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval) is accepted <ins>NeurIPS 2019</ins>! You can find a 8-min video summary [here](https://www.youtube.com/watch?v=wfb9UV_n8jg&t)
