---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!-- 
I'm a 4th year PhD student at the Quebec Artificial Intelligence Institute [(Mila)](https://mila.quebec/) and an intern at [ElementAI](https://www.elementai.com/) under the supervision of [Laurent Charlin](http://www.cs.toronto.edu/~lcharlin/) and [Pau Rodriguez](https://prlz77.github.io/), respectively. 

I'm interested in algorithms able to accumulate transferable knowledge or skills enabling generalization to future tasks. Accordingly, my research topics lie in continual learning and meta-learning. My [recent work](https://arxiv.org/abs/2003.05856) proposes a new and more realistic approach to continual learning at the intersection of both fields.

Recently, I have developed a particular interest in the idea of *composing existing skills to learn new ones quickly*. I believe this is the real appeal of continual learning and that it can propel reinforcement learning. Consequently, i'm currently focused on continual RL. -->


I am a fifth-year Ph.D. student at the Quebec Artificial Intelligence Institute (Mila) and an intern at Amazon Science under  [Laurent Charlin](http://www.cs.toronto.edu/~lcharlin/) and [Rasool Fakoor](https://sites.google.com/site/rfakoor), respectively. Formerly, i've interned at [ElementAI](https://www.elementai.com/) (now [ServiveNow](https://www.servicenow.com/)) and [Spotify Research](https://research.atspotify.com/). 

Iâ€™m interested in algorithms able to accumulate transferable knowledge or skills enabling better and faster generalization to future tasks. Accordingly, my research topics lie in continual learning and meta-learning. Recently, I have developed a particular interest in the idea of *composing existing skills to learn new ones quickly*. I believe this to be the natural appeal of continual learning as it can allow curriculum learning to deliver on its promises, as well as propel reinforcement learning and robotics forward. I'm thus currently focusing on continual RL problems.

Also, checkout our software to unify continual-learning research, [Sequoia](https://github.com/lebrice/Sequoia), as well as my [continual-learning wiki](https://github.com/optimass/continual_learning_papers).


# News

* **09/2021** Our work digging into gradient sparsity for meta and continual learning is accepted at NeurIPS 2021! (stay tuned for the paper)

* **09/2021** Our work solving the task-inference problem in compositonal continual learning is accepted at NeurIPS 2021! (stay tuned for the paper)

* **07/2021** Our work introducing [DiVE](https://arxiv.org/abs/2103.10226), a counterfactual explanation method that goes beyond generating trivial counterfactuals, is accepted at ICCV 2021! 

* **09/2020** Our work proposing a new approach to continual learning [(OSAKA)](https://arxiv.org/abs/2003.05856) is accepted at NeurIPS 2020! 

* **09/2020** Our work proposing a synthetic dataset generator [(Synbols)](https://arxiv.org/abs/2009.06415) to probe different learning algorithms is accepted at NeurIPS 2020!

* **08/2020** I gave [a talk](https://youtu.be/AHGiF21WZbw) on our new approach to continual-learning evaluation towards real-life deployment of continual-learning systems [(OSAKA)](https://arxiv.org/abs/2003.05856) at [ContinualAI](https://www.continualai.org/)

* **06/2020** I hosted a panel with Chelsea Finn, Chris Kanan and Subutai Ahmad at our Continual Learning workshop at CVPR 2020! You can find it [here](https://www.youtube.com/watch?v=sp3Y9Np25Og&t)

* **06/2020** Our work on [online continual compression](https://arxiv.org/abs/1911.08019) lead by [my brother](https://www.cs.mcgill.ca/~lpagec/) is accepted ICML 2020! You can find a 18-min video [here](https://icml.cc/virtual/2020/poster/6338)

* **12/2019** Our workshop on [Continual Learning at CVPR2020](https://sites.google.com/view/clvision2020) as been accepted! Watch out for our really cool competition.

* **12/2019** Our work on [Language GANs Falling Short](https://arxiv.org/abs/1811.02549) was accepted ICLR 2020! You can find a 5-min video summary [here](https://iclr.cc/virtual_2020/poster_BJgza6VtPB.html)

* **09/2019** Our work on [Online Continual learning with Maximal Interfered Retrieval](http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval) is accepted NeurIPS 2019! You can find a 8-min video summary [here](https://www.youtube.com/watch?v=wfb9UV_n8jg&t)




