---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I recently completed my Ph.D. at the Quebec Artificial Intelligence Institute ([Mila](https://mila.quebec/en/)) under the guidance of Professor [Laurent Charlin](http://www.cs.toronto.edu/~lcharlin/). 
My academic journey has been marked by collaborations with globally renowned institutions, including [DeepMind](https://deepmind.com/), where I worked within the Continual Learning team led by [Marc'Aurelio Ranzato](https://ranzato.github.io/), [Amazon](https://www.amazon.ca/) in [Alex Smola](https://alex.smola.org/)'s team, and now at [ServiveNow](https://www.servicenow.com/) as a visiting researcher. I also had the privilege of contributing to ElementAI before its integration with ServiceNow.
 
At the heart of my research is the development of algorithms proficient in accumulating and transferring knowledge or skills to enhance generalization across varied tasks. 
My passion for data and computational efficiency has directed my studies into continual, transfer, and meta-learning, with a particular emphasis on applications spanning language, vision, and reinforcement learning.
 
Currently, at ServiceNow, I'm applying these principles by working with pre-trained Large Language Models (LLMs) to create computer task-solving agents. These agents, leveraging the vast knowledge and adaptability of LLMs, aim to navigate and master various computer tasks more efficiently. This work represents an exciting step towards enhancing the capabilities of automated solutions in different sectors.


Also, check out our software to unify continual-learning research, [Sequoia](https://github.com/lebrice/Sequoia), as well as my [continual-learning wiki](https://github.com/optimass/continual_learning_papers).


# News

* **07/2023** Our new large-scale continual learning benchmark [Nevis'22](https://jmlr.org/papers/v24/22-1345.html) was accepted at <ins>JMLR</ins>! Work done while interning at DeepMind. 

* **05/2023** Our work on [task-agnostic continual RL](https://arxiv.org/abs/2205.14495) is accepted at  <ins>CoLLAs 2023</ins>! Checkout the [5-min video summary](https://www.youtube.com/watch?v=T0RYCuECAuw&t). 

* **12/2021** Our [comparative study of large language models in continual learning](https://openreview.net/forum?id=figzpGMrdD) is accepted at <ins>ICLR 2022</ins>!

* **09/2021** Our work digging into gradient sparsity for meta and continual learning, [Sparse-MAML](https://proceedings.neurips.cc/paper/2021/hash/2a10665525774fa2501c2c8c4985ce61-Abstract.html), is accepted at <ins>NeurIPS 2021</ins>!

* **09/2021** Our work solving the task-inference problem in compositonal continual learning, [Local Modle Composition](https://proceedings.neurips.cc/paper/2021/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html), is accepted at <ins>NeurIPS 2021</ins>!

* **07/2021** Our work introducing [DiVE](https://arxiv.org/abs/2103.10226), a counterfactual explanation method that goes beyond generating trivial counterfactuals, is accepted at <ins>ICCV 2021</ins>! 

* **09/2020** Our work proposing a new approach to continual learning [(OSAKA)](https://arxiv.org/abs/2003.05856) is accepted at <ins>NeurIPS 2020</ins>! 

* **09/2020** Our work proposing a synthetic dataset generator [(Synbols)](https://arxiv.org/abs/2009.06415) to probe different learning algorithms is accepted at <ins>NeurIPS 2020</ins>!

* **08/2020** I gave [a talk](https://youtu.be/AHGiF21WZbw) on our new approach to continual-learning evaluation towards real-life deployment of continual-learning systems [(OSAKA)](https://arxiv.org/abs/2003.05856) at [ContinualAI](https://www.continualai.org/)

* **06/2020** I hosted a panel with Chelsea Finn, Chris Kanan and Subutai Ahmad at our Continual Learning workshop at CVPR 2020! You can find it [here](https://www.youtube.com/watch?v=sp3Y9Np25Og&t)

* **06/2020** Our work on [online continual compression](https://arxiv.org/abs/1911.08019) lead by [my brother](https://www.cs.mcgill.ca/~lpagec/) is accepted <ins>ICML 2020</ins>! You can find a 18-min video [here](https://icml.cc/virtual/2020/poster/6338)

* **12/2019** Our workshop on [Continual Learning at CVPR2020](https://sites.google.com/view/clvision2020) as been accepted! Watch out for our really cool competition.

* **12/2019** Our work on [Language GANs Falling Short](https://arxiv.org/abs/1811.02549) was accepted <ins>ICLR 2020</ins>! You can find a 5-min video summary [here](https://iclr.cc/virtual_2020/poster_BJgza6VtPB.html)

* **09/2019** Our work on [Online Continual learning with Maximal Interfered Retrieval](http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval) is accepted <ins>NeurIPS 2019</ins>! You can find a 8-min video summary [here](https://www.youtube.com/watch?v=wfb9UV_n8jg&t)
